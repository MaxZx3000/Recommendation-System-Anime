# -*- coding: utf-8 -*-
"""Submission_Rekomendasi_Anime.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dV82VZ5LGTC4ZS_ZPud4TcCi37ctAFVB

# Submission Sistem Rekomendasi Anime

Nama: Anthony Kevin Oktavius

Library Imports
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import gensim
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import requests
from google.colab import files
from scipy.stats.stats import kendalltau
from pylab import rcParams
# %matplotlib inline

"""Kaggle Upload"""

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('Kaggle.json has been uploaded!')

!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

"""Dataset Download"""

!kaggle datasets download -d CooperUnion/anime-recommendations-database
!unzip -o anime-recommendations-database.zip

"""Anime Dataset Preview"""

anime_dataset_name = "anime.csv"
anime_df = pd.read_csv(anime_dataset_name)
anime_df

"""Rating Dataset Preview"""

anime_rating_name = "rating.csv"
anime_rating_df = pd.read_csv(anime_rating_name)
anime_rating_df

"""Inisialisasi field pada masing-masing data, agar field-field tersebut lebih mudah dikelola."""

# Field yang ada pada kedua dataset
anime_id_field = "anime_id"
rating_field = "rating"


# Field-field pada rating
name_field = "name"
genre_field = "genre"
type_field = "type"
episodes_field = "episodes"
rating_field = "rating"
members_field = "members"

# Field-field pada rating dataset
user_id_field = "user_id"
user_rating_field = "user_rating"

"""## Exploratory Data Analysis

Definisi fungsi untuk kebutuhan eksplorasi data
"""

def plot_continous_data(plot_title, x_title, y_title, data):
  plt.plot(data)
  plt.title(plot_title)
  plt.xlabel(x_title)
  plt.ylabel(y_title)
  plt.show()

def dynamically_increment_update(dictionary, key, increment_value):
   try:
      dictionary[key] = dictionary[key] + increment_value
   except KeyError:
      dictionary[key] = increment_value
  
   return dictionary

"""### Exploratory Data Analysis Anime

#### Variable Analysis

Melihat deskripsi data dengan numpy dengan info() dan describe(). 

* **info()**: menjelaskan jumlah kolom yang non-null dan data type.
    
* **describe()**: menjelaskan beberapa informasi mengenai statistika suatu data.
"""

anime_df_analysis = anime_df

"""Melihat jumlah kolom yang non-null dan data type """

anime_df_analysis.info()

anime_df_analysis.describe()

"""#### Missing Value

Menghapus data yang null, karena kita masih punya 12017 data.
"""

anime_df_analysis.dropna(inplace=True)
anime_df_analysis.info()

"""Melihat jumlah episode yang "unknown""""

anime_df_analysis[anime_df_analysis[episodes_field].str.contains("Unknown")].sum()

"""Hapus field-field episodes yang memiliki data "Unknown""""

anime_df_analysis = anime_df_analysis.drop(anime_df_analysis[anime_df_analysis[episodes_field].str.contains("Unknown")].index)

"""Ubah field episodes menjadi tipe float64"""

anime_df_analysis[episodes_field] = anime_df_analysis[episodes_field].astype(
    'float64'
)

"""Menjelaskan kembali beberapa informasi mengenai statistika pada data anime."""

anime_df_analysis.describe()

"""#### Eskplorasi Data Genre"""

genres_anime_data = anime_df_analysis[genre_field]
genres_anime_data

number_per_genre = dict()

for i in range(len(genres_anime_data)):
  genre_anime_data = genres_anime_data.iloc[[i][0]]
  splitted_text = genre_anime_data.split(', ')

  # Mendapatkan masing-masing genre
  for i in range(len(splitted_text)):
    number_per_genre = dynamically_increment_update(number_per_genre, 
                                                    splitted_text[i], 
                                                    1)

key_genre_dictionary = list(number_per_genre.keys())
value_genre_dictionary = list(number_per_genre.values())

"""**Visualisasi Data Genre yang ada dengan menggunakan tabel**

Melihat jumlah data pada genre individual.
"""

count_field = "Count"

anime_genre_dataframe = pd.DataFrame(
    data = value_genre_dictionary,
    index = key_genre_dictionary,
    columns = [count_field]
)

print(f"Total anime genres: {anime_genre_dataframe.shape[0]}")
anime_genre_dataframe.sort_values(by=count_field, ascending=False)

"""#### Explorasi Data Type

Melakukan persebaran data type dengan menggunakan value_counts. Lalu, saya menggunakan plot bar untuk melihat persebaran tipe anime.
"""

anime_types_data = anime_df_analysis[type_field]
anime_types_data

count_anime_types = anime_types_data.value_counts()

count_anime_types.plot(kind='bar', title = "Anime Types")

"""#### Ekplorasi Data Episodes

Melihat persebaran data episode dengan menggunakan plot histogram.
"""

anime_df[episodes_field].hist(bins=50)

"""#### Eksplorasi Data Rating

Visualisasi Data Rating
"""

anime_df[rating_field].hist(bins=200)

"""### Exploratory Data Analysis Rating

#### Variable Analysis

Melihat jumlah kolom yang non-null dan data type
"""

anime_rating_analysis_df = anime_rating_df
anime_rating_analysis_df.info()

"""Menjelaskan beberapa informasi mengenai statistika pada data rating."""

anime_rating_analysis_df.describe()

"""##### Eksplorasi Data Rating

Melihat persebaran data anime rating dengan menggunakan plot histogram.


"""

anime_rating_analysis_df[rating_field].hist()

"""Menghapus rating yang memiliki rating -1. -1 artinya adalah user belum melakukan rating sama sekali."""

anomaly_ratings_df = anime_rating_analysis_df[anime_rating_analysis_df[rating_field] == -1]

print(f"Total number of -1: {len(anomaly_ratings_df)}")

anime_rating_analysis_df.drop(
    anomaly_ratings_df.index, 
    inplace=True)

print(len(anime_rating_analysis_df))

"""### Melihat jumlah user, place, dan rating yang tidak duplikat pada dataset rating"""

print("Unique Data:")

print(f"Total user_id: {len(anime_rating_analysis_df[user_id_field].unique())}")
print(f"Total anime_id: {len(anime_rating_analysis_df[anime_id_field].unique())}")
print(f"Total rating: {len(anime_rating_analysis_df[rating_field].unique())}")

"""## Data Preparation dan Modelling"""

anime_rating_cbf_df = anime_rating_analysis_df
anime_cbf_df = anime_df_analysis

"""### Content based filtering Cosine Similarity dan TF IDF Vectorizer

* **TF IDF Vectorizer:**

  TD-IDF (Term Frequency - Inverse Document Frequency) mengukur seberapa pentingnya suatu kata terhadap kata-kata lain dalam dokumen. Ia dibagi menjadi 2 komponen, yaitu TF dan IDF.

  * TF mengukur seberapa sering kata muncul dalam teks tertentu. Teks yang berbeda dalam dokumen mungkin panjangnya berbeda, karena ada faktor dari panjang dokumen. Teks yang berbeda memiliki panjang yang berbeda. Maka dari itu, harus dinormalisasi dengan membagi jumlah kemunculan terhadap panjang dokumen.

  * IDF mengukur pentingnya suatu istilah di dalam korpus. Ia memastikan agar kepentingan suatu kata di dalam kalimat itu sama. Misalkan stop words seperti is, am, are, dsb sering muncul di suatu kalimat, namun mereka tidak penting kehadirannya. Maka dari itu, semua kata dinormalisasikan agar setiap kata bisa mendapatkan nilai kepentingan yang fair.

  Dari kesimpulan di atas, kita bisa mendapatkan rumus sebagai berikut.

  ![Rumus TF-IDF](https://cdn-media-1.freecodecamp.org/images/1*nSqHXwOIJ2fa_EFLTh5KYw.png)

  Dalam fungsi ini, kita bisa menggunakan TfIdfVectorizer. TFIdfVectorizer adalah salah satu fungsi yang disediakan dari sklearn.

* **Cosine Similarity:**

  Cosine similarity adalah salah satu metrik untuk mengukur kesamaan. Cosine similarity mengukur kesamaan dua vektor dengan menentukan apakah kedua vektornya sudah mengarah kepada arah yang sama. Semakin kecil sudut cosinus, semakin besar nilai cosine similarity.

  Metrik ini sering digunakan untuk analisis teks. Dalam hal ini, dari hasil TF IDF, kita akan mengukur kesamaan antar satu item dengan fitur yang ada pada item tersebut, sehingga kita bisa merekomendasikan anime yang cocok dengan genre yang diberikan.

  Rumusnya adalah sebagai berikut.

    ![Rumus Cosine Similarity](https://neo4j.com/docs/graph-data-science/current/_images/cosine-similarity.png)

"""

anime_genre_cbf_df = anime_cbf_df[genre_field].to_list()
anime_genre_cbf_df[:5]

"""#### Data Preparation dan Training Model

**Data Preparation:**

Melakukan splitting value ketika ada tanda koma (,), agar kita bisa merekomendasikan anime yang memiliki genre tambahan selain genre yang ditentukan.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

def split_token(value):
  return value.split(', ')

tf_idf_vectorizer = TfidfVectorizer(tokenizer = split_token)

tf_idf_vectorizer.fit(anime_genre_cbf_df)

tf_idf_vectorizer.get_feature_names()

"""Mengecek ukuran bentuk matriks pada data anime_id dan genre."""

tf_idf_matrix = tf_idf_vectorizer.fit_transform(anime_genre_cbf_df)

tf_idf_matrix.shape

"""Melakukan testing dengan data pada indeks pertama"""

tf_idf_matrix.todense()[0]

"""Mengukur kesamaan antar elemen yang satu dengan elemen yang lain, dengan menggunakan cosine similarity"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_similarity_value = cosine_similarity(tf_idf_matrix)
cosine_similarity_value

cosine_similarity_df = pd.DataFrame(
    cosine_similarity_value,
    index = anime_cbf_df[name_field],
    columns = anime_cbf_df[name_field]
)

cosine_similarity_df

"""#### Melakukan Top N Recommendation

Mulai melakukan rekomendasi pada data anime
"""

def get_anime_recommendations(anime_name,
                              similarity_data = cosine_similarity_df,
                              items = anime_cbf_df[[name_field, genre_field]],
                              k = 4):
  
  print(range(-1, -k, -1))

  index = similarity_data.loc[:, anime_name].to_numpy().argpartition(
      range(-1, -k, -1)
  )

  closest = similarity_data.columns[index[-1:-(k+2):-1]]

  return pd.DataFrame(closest).merge(items).head(k)

"""Melihat contoh data yang memiliki genre Drama"""

anime_cbf_df[anime_cbf_df[genre_field].str.contains('Drama')]

"""Mendapatkan top N rekomendasi"""

anime_recommendations = get_anime_recommendations('Kimi no Na wa.')
anime_recommendations

"""#### Melakukan evaluasi dengan metrik akurasi."""

def get_accuracy_content_based_filtering_metrics(df, 
                                                 comparation_index, 
                                                 field_to_be_compared,
                                                 expected_name):

  number_of_rows = df.shape[0]
  one_correct_value = 100.0 / len(df)
  total_accuracy_score = 0

  for index in range(number_of_rows):
    if index == comparation_index:
      continue

    if expected_name in df[field_to_be_compared][index]:
      total_accuracy_score += one_correct_value

  return total_accuracy_score

genre_accuracy_score = get_accuracy_content_based_filtering_metrics(anime_recommendations,
                                                                    -1,
                                                                    genre_field,
                                                                    "Drama")

print(f"Accuracy Score: {genre_accuracy_score}%")

"""### Collaborative Filtering dengan SVD (Matrix Factorization)


SVD adalah salah satu teknik yang digunakan untuk dimensionality reduction. Teknik ini bekerja dengan mengurangi jumlah fitur yang ada pada dataset (mirip dengan PCA) dengan jumlah fitur yang ditentukan oleh user.

Namun, jika kita berbicara konteks sistem rekomendasi, SVD menggunakan sturktur matriks. Berikut adalah bagian-bagian dari SVD, jika menggunakan struktur matriks. 

  * Bagian row menunjukkan user
  * Bagian column menunjukkan item
  * Elemen-elemen pada matriks menunjukkan rating

Dari matriks di atas, SVD mengurangi dimensi matriks terebut dengan faktor-faktor laten. Ia melakukan mapping setiap fitur dan item menjadi matriks yang lebih kecil. Matriks inilah yang memiliki informasi mengenai relasi user dan item.

Rumus untuk SVD adalah sebagai berikut.

![Rumus SVD](https://miro.medium.com/max/894/1*XNWUlrQJXGeoCDqUMd0iUA.png)

Untuk melakukan training pada SVD, ia menggunakan teknik SGD (Schocastic Gradient Descent). SGD bekerja dengan meminimalkan nilai awal dan melakukan iterasi untuk mengurangi kesalahan antara nilai yang diprediksi dan nilai aktual. Mirip seperti SGD pada klasifikasi, ia akan mengoptimalkan cost function agar bisa konvergensi ke titik global optima.

Impor Library yang akan digunakan untuk collaborative filtering
"""

!pip install surprise
import surprise
from surprise import SVD
from surprise import accuracy
from sklearn.model_selection import train_test_split
from surprise.model_selection import cross_validate
from collections import defaultdict

"""#### Data Preparation"""

anime_rating_cbf_df

"""**Standardisasi Nilai Rating**

Melakukan standardisasi pada nilai rating, agar machine learning bisa lebih cepat mempelajari data (terutama jika berbicara mengenai jarak dan gradient descent), dengan skala antara 0 - 1.
"""

anime_rating_cbf_df[rating_field] = anime_rating_cbf_df[rating_field].apply(
    lambda x: (x - 1) / (10 - 1)
).values
anime_rating_cbf_df.describe()

"""**Train Tets Split**

Melakukan train test split, dengan proporsi sebagai berikut:

* **Train:** 80%
* **Test:** 20%

Tujuan dari pembagian data ini adalah agar kita bisa mengukur kemampuan machine learning dengan data yang belum pernah dipelajari. 


"""

anime_rating_train_data, anime_rating_test_data = train_test_split(
    anime_rating_cbf_df,
    test_size = 0.2,
    random_state = 30
)

"""#### Training Model

Melakukan konversi dataframe menjadi data yang bisa dibaca dengan library surprise.
"""

data_reader = surprise.Reader(rating_scale=(0,1))

train_surprise_data = surprise.Dataset.load_from_df(
    anime_rating_train_data, 
    data_reader
)

test_surprise_data = surprise.Dataset.load_from_df(
    anime_rating_test_data, 
    data_reader
)

anime_trainset_surprise = train_surprise_data.build_full_trainset()

"""Collaborative Filtering menggunakan SVD (Matrix Facrorization Technique). Di sini, saya akan menggunakan attribut pada genre"""

svd_classifier = SVD(n_epochs = 3, verbose=True)
anime_predictor_surprise = svd_classifier.fit(anime_trainset_surprise)

"""#### Evaluasi Model

Dalam collaborative filtering, terdapat dua metrik yang saya gunakan, yaitu MAE dan RMSE. 

* **MAE**: rata-rata kesalahan absolut antara data sebenarnya dengan data yang diprediksikan. Rumusnya adalah sebagai berikut:

  ![Rumus MAE](https://www.statisticshowto.com/wp-content/uploads/2016/10/MAE.png)

* **RMSE**: akar kuadrat dari MSE. MSE sendiri adalah kuadrat dari rata-rata kesalahan. Rumusnya adalah sebagai berikut.

  ![Rumus RMSE](https://1.bp.blogspot.com/-AodtifmdR1U/X-NOXo0avGI/AAAAAAAACmI/_jvy7eLB72UB00dW_buPYZCa9ST2yx8XACNcBGAsYHQ/s453/rumus%2Brmse.jpg)

Mendapatkan nilai MAE dan RMSE
"""

cross_validate(svd_classifier, 
               train_surprise_data, 
               measures = ['RMSE', 'MSE'], 
               cv = 3,
               verbose = True)

"""Melakukan prediksi dengan data testing"""

anime_trainset_test_surprise = test_surprise_data.build_full_trainset()
anime_surprise_testset = anime_trainset_test_surprise.build_testset()

predictions = anime_predictor_surprise.test(anime_surprise_testset)

"""#### Melihat top n rekomendasi dengan sampel user id 23435"""

predict_user_id = 23435

"""Melihat hasil prediksi rating dari user-user pada data testing"""

for index, prediction in enumerate(predictions):
  if prediction[0] == predict_user_id:
    print(prediction)

"""Melakukan rekomendasi anime"""

def get_top_n_from_specific_user_id(predictions, 
                                    number_of_items,
                                    user_id,
                                    except_array=[]):
  top_n = defaultdict(list)

  # Membuat pemetaan user terlebih dahulu, dengan melihat terlebih dahulu
  # apakah user tersebut sudah pernah menonton movie tersebut
  for uid, iid, true_r, est, _ in predictions:
    if user_id == uid and iid not in except_array:
      top_n[uid].append((iid, est))

  # Melakukan pengurutan ranking pada user yang diberikan pada argument
  for uid, user_ratings in top_n.items():
    user_ratings.sort(key=lambda x: x[1], reverse=True)
    top_n[uid] = user_ratings[:number_of_items]

  return top_n

watched_animes = anime_rating_cbf_df[anime_rating_cbf_df[user_id_field] == predict_user_id]
top_n_animes = get_top_n_from_specific_user_id(predictions,
                                               number_of_items = 5,
                                               user_id = predict_user_id,
                                               except_array = watched_animes)

top_n_animes = top_n_animes[predict_user_id]

print(f"Recommendation for user id: {predict_user_id}")

recommended_animes = []

for i, recommended_anime_id in enumerate(top_n_animes):
  recommended_animes.append(recommended_anime_id[0])

recommended_anime_names = anime_cbf_df[anime_cbf_df[anime_id_field].isin(recommended_animes)]
recommended_anime_names